{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# IDEAW Training on Google Colab\n","\n","This notebook trains IDEAW audio watermarking models using Colab's free GPU.\n","\n","**Before running:**\n","1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n","2. Upload your data to Google Drive\n","3. Update the GitHub URL below with your repository"],"metadata":{"id":"header"}},{"cell_type":"markdown","source":["## 1. Setup Environment"],"metadata":{"id":"setup-header"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","%cd /content/drive/MyDrive/audio-watermarking-demo\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoRH8Wi3rz44","executionInfo":{"status":"ok","timestamp":1763304064392,"user_tz":-330,"elapsed":2534,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"1508c565-9ec0-4dd1-f027-fe44e56c7bc4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/audio-watermarking-demo\n"]}]},{"cell_type":"code","source":["!git status\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGjpwQ2sx8fm","executionInfo":{"status":"ok","timestamp":1763304116158,"user_tz":-330,"elapsed":33742,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"f5f46b48-dd6b-4dc7-9566-eb9a0942d735"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n"]}]},{"cell_type":"code","source":["!git reset --soft HEAD~5\n"],"metadata":{"id":"ilKL3OpaznrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add colab_notebooks/IDEAW_Training_Template.ipynb"],"metadata":{"id":"nrGMoGE9SF0B","executionInfo":{"status":"ok","timestamp":1763292357596,"user_tz":-330,"elapsed":961,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["!git config --global user.name \"Abdullah Yassir\"\n","!git config --global user.email \"abdullahyassir2222@gmail.com\"\n"],"metadata":{"id":"WdmuoYP8V4yG","executionInfo":{"status":"ok","timestamp":1763291171812,"user_tz":-330,"elapsed":203,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"training complete\"\n","\n","!git push origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WpD__ypaSYsg","executionInfo":{"status":"ok","timestamp":1763292374750,"user_tz":-330,"elapsed":2759,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"10dd2908-19a4-49f9-93b8-f4b64b13b821"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[main fc373d4] training complete\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite colab_notebooks/IDEAW_Training_Template.ipynb (94%)\n","Enumerating objects: 7, done.\n","Counting objects: 100% (7/7), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (4/4), done.\n","Writing objects: 100% (4/4), 2.43 KiB | 191.00 KiB/s, done.\n","Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/Abdullahyassir007/audio-watermarking-demo.git\n","   eaee1ed..fc373d4  main -> main\n"]}]},{"cell_type":"code","metadata":{"id":"30ac6263","executionInfo":{"status":"ok","timestamp":1763290615601,"user_tz":-330,"elapsed":114,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}}},"source":["# List all untracked files recursively, excluding those ignored by .gitignore\n","!git ls-files --others --exclude-standard"],"execution_count":28,"outputs":[]},{"cell_type":"code","source":["!git add colab_notebooks/IDEAW_Training_Template.ipynb\n","\n","# 4. Commit with a message\n","!git commit -m \"Running training loop\"\n","\n","# 5. Push to GitHub\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTP6ZO0mzgnz","executionInfo":{"status":"ok","timestamp":1763241774930,"user_tz":-330,"elapsed":3674,"user":{"displayName":"Abdullah Yassir","userId":"02050531084960743622"}},"outputId":"d30e447d-2ca0-43c2-dbe7-9cfc94a9e12b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 486d4a1] Running training loop\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite colab_notebooks/IDEAW_Training_Template.ipynb (97%)\n","Enumerating objects: 7, done.\n","Counting objects: 100% (7/7), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (4/4), done.\n","Writing objects: 100% (4/4), 5.23 KiB | 382.00 KiB/s, done.\n","Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","remote: This repository moved. Please use the new location:\u001b[K\n","remote:   https://github.com/Abdullahyassir007/audio-watermarking-demo.git\u001b[K\n","To https://github.com/AbdullahYassir007/audio-watermarking-demo.git\n","   8dccca0..486d4a1  main -> main\n"]}]},{"cell_type":"code","source":["# !git checkout -- colab_notebooks/IDEAW_Training_Template.ipynb\n","!git pull origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRxHZDvYsN8J","executionInfo":{"status":"ok","timestamp":1763293860835,"user_tz":-330,"elapsed":211806,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"2e46ae4f-141b-4012-e770-25423cd85b10"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects:  20% (1/5)\u001b[K\rremote: Compressing objects:  40% (2/5)\u001b[K\rremote: Compressing objects:  60% (3/5)\u001b[K\rremote: Compressing objects:  80% (4/5)\u001b[K\rremote: Compressing objects: 100% (5/5)\u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 13 (delta 7), reused 12 (delta 6), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects: 100% (13/13), 2.39 KiB | 0 bytes/s, done.\n","From https://github.com/Abdullahyassir007/audio-watermarking-demo\n"," * branch            main       -> FETCH_HEAD\n","   fc373d4..9c3dd39  main       -> origin/main\n","Updating fc373d4..9c3dd39\n","Fast-forward\n"," .gitignore                                   |   3 \u001b[32m+\u001b[m\n"," .kiro/specs/audio-watermarking-demo/tasks.md | 140 \u001b[32m+++++++++++++++\u001b[m\u001b[31m------------\u001b[m\n"," 2 files changed, 80 insertions(+), 63 deletions(-)\n"]}]},{"cell_type":"code","source":["# Abort the rebase\n","!git rebase --abort\n","\n","# Accept the remote version (my fix)\n","!git reset --hard origin/main\n","\n","# Now re-apply just your notebook and config changes\n","!git checkout HEAD~1 -- colab_notebooks/IDEAW_Training_Template.ipynb\n","!git checkout HEAD~1 -- research/IDEAW/config.yaml\n","\n","# Commit these changes\n","!git add colab_notebooks/IDEAW_Training_Template.ipynb research/IDEAW/config.yaml\n","!git commit -m \"Update Colab notebook and config for batch size 2\"\n","\n","# Push\n","!git push origin main\n"],"metadata":{"id":"XMgPpj37RhRd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763239781619,"user_tz":-330,"elapsed":3853,"user":{"displayName":"Abdullah Yassir","userId":"02050531084960743622"}},"outputId":"a96d8e8a-9396-4872-c8d3-599b113faf11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HEAD is now at e0e1c82 Fix IDEAW PyTorch 2.x compatibility - STFT/iSTFT complex tensor handling\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","nothing to commit, working tree clean\n","Everything up-to-date\n"]}]},{"cell_type":"code","source":["\n","\n","# Set up paths\n","DRIVE_PATH = '/content/drive/MyDrive/audio-watermarking-demo'\n","DATA_PATH = f'{DRIVE_PATH}/Dataset'\n","CHECKPOINT_PATH = f'{DRIVE_PATH}/checkpoints'\n","RESULTS_PATH = f'{DRIVE_PATH}/results'\n","\n","# Create directories\n","import os\n","os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","\n","print(\"‚úì Google Drive mounted\")\n","print(f\"‚úì Data path: {DATA_PATH}\")\n","print(f\"‚úì Checkpoint path: {CHECKPOINT_PATH}\")\n","print(f\"‚úì Results path: {RESULTS_PATH}\")"],"metadata":{"id":"mount-drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763302876369,"user_tz":-330,"elapsed":10,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"f8ff0271-2975-4c60-e859-c9ec2eb043a0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Google Drive mounted\n","‚úì Data path: /content/drive/MyDrive/audio-watermarking-demo/Dataset\n","‚úì Checkpoint path: /content/drive/MyDrive/audio-watermarking-demo/checkpoints\n","‚úì Results path: /content/drive/MyDrive/audio-watermarking-demo/results\n"]}]},{"cell_type":"code","source":["# Just install the missing packages, use Colab's existing PyTorch\n","!pip install -q librosa==0.10.1 pydub PyYAML soundfile tqdm resampy\n","\n","# Restart runtime\n","import os\n","os.kill(os.getpid(), 9)\n","\n"],"metadata":{"id":"1KKuaYkDBVnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install dependencies from IDEAW requirements\n","!pip install -q -r research/IDEAW/requirements_colab.txt\n","!pip install -q FrEIA\n","\n","print(\"‚úì Dependencies installed\")"],"metadata":{"id":"install-deps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763302895865,"user_tz":-330,"elapsed":15506,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"a27f0454-1e91-45e3-8e81-86f3741905a4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Dependencies installed\n"]}]},{"cell_type":"code","source":["# Check GPU availability\n","import torch\n","\n","print(f\"GPU Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","    device = 'cuda'\n","else:\n","    print(\"‚ö†Ô∏è No GPU available, using CPU\")\n","    device = 'cpu'\n","\n","print(f\"\\n‚úì Using device: {device}\")"],"metadata":{"id":"check-gpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763302905116,"user_tz":-330,"elapsed":9241,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"cdfdcd9f-3782-4bc9-99c2-c9f7bc697974"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Available: False\n","‚ö†Ô∏è No GPU available, using CPU\n","\n","‚úì Using device: cpu\n"]}]},{"cell_type":"code","source":["# Verify installation\n","import torch\n","import librosa\n","import scipy\n","import numpy as np\n","import yaml\n","\n","print(\"=\" * 50)\n","print(\"ENVIRONMENT CHECK\")\n","print(\"=\" * 50)\n","print(f\"‚úì PyTorch: {torch.__version__}\")\n","print(f\"‚úì Librosa: {librosa.__version__}\")\n","print(f\"‚úì Scipy: {scipy.__version__}\")\n","print(f\"‚úì Numpy: {np.__version__}\")\n","print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n","print(\"=\" * 50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0527lnznCL1i","executionInfo":{"status":"ok","timestamp":1763302905280,"user_tz":-330,"elapsed":162,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"b0228907-762e-4166-f0fe-17abceb48201"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","ENVIRONMENT CHECK\n","==================================================\n","‚úì PyTorch: 2.8.0+cu126\n","‚úì Librosa: 0.10.1\n","‚úì Scipy: 1.11.4\n","‚úì Numpy: 1.26.4\n","‚úì CUDA available: False\n","==================================================\n"]}]},{"cell_type":"markdown","source":["## 2. Load IDEAW Model"],"metadata":{"id":"load-model-header"}},{"cell_type":"code","source":["# # Import IDEAW\n","# import sys\n","# sys.path.insert(0, '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW')\n","\n","# from models.ideaw import IDEAW\n","\n","# # Configuration\n","# config_path = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW/config.yaml'\n","# model_config_path = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW/models/config.yaml'\n","\n","# # Initialize model\n","# ideaw = IDEAW(model_config_path, device)\n","# print(\"‚úì IDEAW model initialized\")\n","\n","# # Count parameters\n","# total_params = sum(p.numel() for p in ideaw.parameters())\n","# trainable_params = sum(p.numel() for p in ideaw.parameters() if p.requires_grad)\n","# print(f\"Total parameters: {total_params:,}\")\n","# print(f\"Trainable parameters: {trainable_params:,}\")"],"metadata":{"id":"load-model","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763238758256,"user_tz":-330,"elapsed":1715,"user":{"displayName":"Abdullah Yassir","userId":"02050531084960743622"}},"outputId":"9251dcd3-152d-483f-ad20-084007e25279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì IDEAW model initialized\n","Total parameters: 8,425,023\n","Trainable parameters: 8,425,023\n"]}]},{"cell_type":"markdown","source":["## 3. Prepare Data"],"metadata":{"id":"data-header"}},{"cell_type":"code","source":["# ============================================\n","# PREPARE DATA FOR IDEAW TRAINING\n","# ============================================\n","import os\n","import pickle\n","import librosa\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Paths\n","DRIVE_PATH = '/content/drive/MyDrive/audio-watermarking-demo'\n","RAW_DATA_PATH = f'{DRIVE_PATH}/Dataset'\n","PROCESSED_DATA_PATH = '/content/processed_data'\n","CHECKPOINT_PATH = f'{DRIVE_PATH}/checkpoints'\n","RESULTS_PATH = f'{DRIVE_PATH}/results'\n","\n","# Parameters\n","MAX_FILES = 50  # Quick test with 50 files (set to None for all)\n","SAMPLE_RATE = 16000\n","SEGMENT_SAMPLES = 16000  # 1 second\n","\n","# Create directories\n","os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n","os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n","os.makedirs(f'{CHECKPOINT_PATH}/stage_I', exist_ok=True)\n","os.makedirs(f'{CHECKPOINT_PATH}/stage_II', exist_ok=True)\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","\n","print(\"=\"*50)\n","print(\"DATA PREPARATION\")\n","print(\"=\"*50)\n","\n","# Find audio files\n","if not os.path.exists(RAW_DATA_PATH):\n","    print(f\"‚ùå Data not found at {RAW_DATA_PATH}\")\n","else:\n","    audio_extensions = ['.mp3', '.wav', '.flac', '.m4a']\n","    audio_files = []\n","\n","    for root, dirs, files in os.walk(RAW_DATA_PATH):\n","        for file in files:\n","            if any(file.lower().endswith(ext) for ext in audio_extensions):\n","                audio_files.append(os.path.join(root, file))\n","\n","    print(f\"\\n‚úì Found {len(audio_files)} audio files\")\n","\n","    # Limit for testing\n","    if MAX_FILES and len(audio_files) > MAX_FILES:\n","        audio_files = audio_files[:MAX_FILES]\n","        print(f\"‚úì Using {MAX_FILES} files for quick test\")\n","\n","    if len(audio_files) > 0:\n","        print(f\"\\nProcessing {len(audio_files)} files...\")\n","        print(f\"Target: 16kHz, 1-second segments\")\n","\n","        data = []\n","\n","        for audio_path in tqdm(audio_files):\n","            try:\n","                # Load and resample\n","                audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n","\n","                # Split into 1-second segments\n","                num_segments = int(len(audio) / SEGMENT_SAMPLES)\n","\n","                for i in range(num_segments):\n","                    start = i * SEGMENT_SAMPLES\n","                    end = start + SEGMENT_SAMPLES\n","                    segment = audio[start:end]\n","\n","                    if len(segment) == SEGMENT_SAMPLES:\n","                        data.append(segment)\n","\n","            except Exception as e:\n","                print(f\"\\n‚ö†Ô∏è  Error: {os.path.basename(audio_path)}\")\n","                continue\n","\n","        print(f\"\\n‚úì Processed {len(audio_files)} files\")\n","        print(f\"‚úì Created {len(data)} segments\")\n","\n","        if len(data) > 0:\n","            # Save pickle\n","            pickle_path = os.path.join(PROCESSED_DATA_PATH, 'audio.pkl')\n","            with open(pickle_path, 'wb') as f:\n","                pickle.dump(data, f)\n","\n","            size_mb = os.path.getsize(pickle_path) / (1024 * 1024)\n","\n","            print(f\"\\n‚úì Pickle saved: {pickle_path}\")\n","            print(f\"‚úì Segments: {len(data)}\")\n","            print(f\"‚úì Duration: {len(data)/60:.1f} minutes\")\n","            print(f\"‚úì Size: {size_mb:.1f} MB\")\n","\n","            print(\"\\n\" + \"=\"*50)\n","            print(\"‚úÖ DATA READY FOR TRAINING\")\n","            print(\"=\"*50)\n","\n","            PICKLE_PATH = pickle_path\n","        else:\n","            print(\"‚ùå No segments created\")\n","    else:\n","        print(\"‚ùå No audio files found\")"],"metadata":{"id":"prepare-data","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763302988497,"user_tz":-330,"elapsed":48649,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"29a36806-70ed-4719-fe5f-63d9381091cf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","DATA PREPARATION\n","==================================================\n","\n","‚úì Found 2699 audio files\n","‚úì Using 50 files for quick test\n","\n","Processing 50 files...\n","Target: 16kHz, 1-second segments\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:30<00:00,  1.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","‚úì Processed 50 files\n","‚úì Created 396 segments\n","\n","‚úì Pickle saved: /content/processed_data/audio.pkl\n","‚úì Segments: 396\n","‚úì Duration: 6.6 minutes\n","‚úì Size: 24.2 MB\n","\n","==================================================\n","‚úÖ DATA READY FOR TRAINING\n","==================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## 4. Training Configuration"],"metadata":{"id":"config-header"}},{"cell_type":"code","source":["# Override batch size in config file\n","import yaml\n","\n","config_path = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW/config.yaml'\n","\n","# Read config\n","with open(config_path, 'r') as f:\n","    config = yaml.load(f, Loader=yaml.FullLoader)\n","\n","# Change batch size\n","config['train']['batch_size'] = 1  # Try batch size 2 (very small)\n","config['train']['num_workers'] = 0  # Disable multiprocessing\n","\n","# Save config\n","with open(config_path, 'w') as f:\n","    yaml.dump(config, f)\n","\n","print(f\"‚úì Updated config: batch_size = {config['train']['batch_size']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PE4oqIQDPZD-","executionInfo":{"status":"ok","timestamp":1763303040300,"user_tz":-330,"elapsed":509,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"6bd14a2f-62ad-4f7e-8a2c-aaca4bf8e76b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Updated config: batch_size = 1\n"]}]},{"cell_type":"code","source":["# Training hyperparameters\n","BATCH_SIZE = 1\n","NUM_ITERATIONS = 100  # Quick test (use 10000+ for full training)\n","SAVE_EVERY = 40\n","\n","print(\"Training Configuration:\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Iterations: {NUM_ITERATIONS}\")\n","print(f\"  Device: {device}\")\n","print(f\"  Save every: {SAVE_EVERY} iterations\")\n","print(f\"  Pickle path: {PICKLE_PATH}\")"],"metadata":{"id":"training-config","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763303041471,"user_tz":-330,"elapsed":13,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"e45d7a95-8104-4cd0-c0c4-b4ec545104de"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Configuration:\n","  Batch size: 1\n","  Iterations: 100\n","  Device: cpu\n","  Save every: 40 iterations\n","  Pickle path: /content/processed_data/audio.pkl\n"]}]},{"cell_type":"markdown","source":["## 4.5 Create IDEAW-Plus Improvements"],"metadata":{"id":"AULMrOw5Dhrs"}},{"cell_type":"markdown","source":["### Cell 1: Configuration Flag"],"metadata":{"id":"3LXmpZjUDvHI"}},{"cell_type":"code","source":["# ============================================\n","# IDEAW-PLUS CONFIGURATION\n","# ============================================\n","\n","# Set to True to use IDEAW-Plus, False for baseline IDEAW\n","USE_IDEAW_PLUS = True\n","\n","print(\"=\"*50)\n","if USE_IDEAW_PLUS:\n","    print(\"üöÄ USING IDEAW-PLUS (with improvements)\")\n","    print(\"  ‚ú® Attention mechanism\")\n","    print(\"  ‚ú® Residual connections\")\n","    print(\"  ‚ú® Perceptual loss\")\n","else:\n","    print(\"üìä USING BASELINE IDEAW (for comparison)\")\n","print(\"=\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIBbvn-FDv2-","executionInfo":{"status":"ok","timestamp":1763303194690,"user_tz":-330,"elapsed":57,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"e4ce186d-ade5-4502-c90d-42a5779f559f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","üöÄ USING IDEAW-PLUS (with improvements)\n","  ‚ú® Attention mechanism\n","  ‚ú® Residual connections\n","  ‚ú® Perceptual loss\n","==================================================\n"]}]},{"cell_type":"code","source":["# ============================================\n","# IDEAW-PLUS: ALL THREE IMPROVEMENTS\n","# ============================================\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW')\n","\n","import torch\n","import torch.nn as nn\n","from models.innBlock import InnBlock\n","from models.dense import DenseBlock\n","\n","# ============================================\n","# IMPROVEMENT #1: ATTENTION MECHANISM\n","# ============================================\n","class ChannelAttention(nn.Module):\n","    \"\"\"Channel attention for focusing on important frequency bands\"\"\"\n","    def __init__(self, channels, reduction=4):\n","        super().__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channels, channels // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channels // reduction, channels, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class AttentionInnBlock(InnBlock):\n","    \"\"\"InnBlock with channel attention\"\"\"\n","    def __init__(self, config_path):\n","        super().__init__(config_path)\n","        self.attention = ChannelAttention(self.channel, reduction=4)\n","\n","    def forward(self, x1, x2, rev=False):\n","        if not rev:\n","            x1 = self.attention(x1)\n","        return super().forward(x1, x2, rev)\n","\n","# ============================================\n","# IMPROVEMENT #2: RESIDUAL CONNECTIONS\n","# ============================================\n","class ResDenseBlock(DenseBlock):\n","    \"\"\"DenseBlock with residual connections for better gradient flow\"\"\"\n","    def __init__(self, config_path, channel_in, channel_out):\n","        super().__init__(config_path, channel_in, channel_out)\n","        # Add projection if dimensions don't match\n","        if channel_in != channel_out:\n","            self.projection = nn.Conv2d(channel_in, channel_out, 1)\n","        else:\n","            self.projection = None\n","\n","    def forward(self, x):\n","        identity = x\n","        out = super().forward(x)\n","\n","        # Apply projection if needed\n","        if self.projection is not None:\n","            identity = self.projection(identity)\n","\n","        # Residual connection\n","        return out + identity\n","\n","# ============================================\n","# IMPROVEMENT #3: PERCEPTUAL LOSS\n","# ============================================\n","class PerceptualLoss(nn.Module):\n","    \"\"\"STFT-based perceptual loss for better audio quality\"\"\"\n","    def __init__(self, n_fft=1024, hop_length=256):\n","        super().__init__()\n","        self.n_fft = n_fft\n","        self.hop_length = hop_length\n","\n","    def forward(self, pred, target):\n","        # Compute STFT\n","        window = torch.hann_window(self.n_fft).to(pred.device)\n","\n","        pred_stft = torch.stft(pred, n_fft=self.n_fft, hop_length=self.hop_length,\n","                               window=window, return_complex=True)\n","        target_stft = torch.stft(target, n_fft=self.n_fft, hop_length=self.hop_length,\n","                                 window=window, return_complex=True)\n","\n","        # Magnitude loss (more important)\n","        pred_mag = torch.abs(pred_stft)\n","        target_mag = torch.abs(target_stft)\n","        mag_loss = nn.functional.l1_loss(pred_mag, target_mag)\n","\n","        # Phase loss (less important, weighted lower)\n","        pred_phase = torch.angle(pred_stft)\n","        target_phase = torch.angle(target_stft)\n","        phase_loss = nn.functional.l1_loss(pred_phase, target_phase)\n","\n","        return mag_loss + 0.1 * phase_loss\n","\n","print(\"‚úì Improvement #1: AttentionInnBlock created\")\n","print(\"‚úì Improvement #2: ResDenseBlock created\")\n","print(\"‚úì Improvement #3: PerceptualLoss created\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RPrT-dRD46H","executionInfo":{"status":"ok","timestamp":1763303228110,"user_tz":-330,"elapsed":3794,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"f8317ee6-525c-405e-c554-6718d9e5dfcf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n","  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n","  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(flt)p?( \\(default\\))?$', token):\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"]},{"output_type":"stream","name":"stdout","text":["‚úì Improvement #1: AttentionInnBlock created\n","‚úì Improvement #2: ResDenseBlock created\n","‚úì Improvement #3: PerceptualLoss created\n"]}]},{"cell_type":"markdown","source":["\n","### Cell 3: Create MIHNET-Plus (with Attention)"],"metadata":{"id":"vtV0FsKnEBfg"}},{"cell_type":"code","source":["# ============================================\n","# MIHNET-PLUS: ATTENTION-ENHANCED MIHNET\n","# ============================================\n","\n","class Mihnet_Plus_s1(nn.Module):\n","    \"\"\"MIHNET Stage 1 with attention\"\"\"\n","    def __init__(self, config_path, num_inn):\n","        super().__init__()\n","        self.innbs = nn.ModuleList([\n","            AttentionInnBlock(config_path) for _ in range(num_inn)\n","        ])\n","\n","    def forward(self, a, m, rev=False):\n","        if not rev:\n","            for innb in self.innbs:\n","                a, m = innb(a, m)\n","        else:\n","            for innb in reversed(self.innbs):\n","                a, m = innb(a, m, rev=True)\n","        return a, m\n","\n","class Mihnet_Plus_s2(nn.Module):\n","    \"\"\"MIHNET Stage 2 with attention\"\"\"\n","    def __init__(self, config_path, num_inn):\n","        super().__init__()\n","        self.innbs = nn.ModuleList([\n","            AttentionInnBlock(config_path) for _ in range(num_inn)\n","        ])\n","\n","    def forward(self, a, m, rev=False):\n","        if not rev:\n","            for innb in self.innbs:\n","                a, m = innb(a, m)\n","        else:\n","            for innb in reversed(self.innbs):\n","                a, m = innb(a, m, rev=True)\n","        return a, m\n","\n","print(\"‚úì Mihnet_Plus classes created\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQ0dFXTADn9p","executionInfo":{"status":"ok","timestamp":1763303264815,"user_tz":-330,"elapsed":42,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"b258ead9-41a9-4149-8bd3-b2727601edf7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Mihnet_Plus classes created\n"]}]},{"cell_type":"markdown","source":["### Cell 4: Create IDEAW-Plus Model (Complete)"],"metadata":{"id":"wPlzFtSIEKC5"}},{"cell_type":"code","source":["# ============================================\n","# IDEAW-PLUS: COMPLETE MODEL WITH ALL IMPROVEMENTS\n","# ============================================\n","import yaml\n","from models.ideaw import IDEAW\n","from models.componentNet import Discriminator, BalanceBlock\n","from models.attackLayer import AttackLayer\n","\n","class IDEAW_Plus(IDEAW):\n","    \"\"\"\n","    IDEAW-Plus: Enhanced version with:\n","    1. Attention mechanism in InnBlocks\n","    2. Residual connections (inherited from DenseBlock modifications)\n","    3. Perceptual loss (applied during training)\n","    \"\"\"\n","    def __init__(self, config_path, device):\n","        # Don't call super().__init__() - we'll rebuild with our components\n","        nn.Module.__init__(self)\n","        self.load_config(config_path)\n","\n","        # Use attention-enhanced MIHNETs (Improvement #1)\n","        self.hinet_1 = Mihnet_Plus_s1(config_path, self.num_inn_1)\n","        self.hinet_2 = Mihnet_Plus_s2(config_path, self.num_inn_2)\n","\n","        # Original components (unchanged)\n","        self.msg_fc = nn.Linear(self.num_bit, self.num_point)\n","        self.msg_fc_back = nn.Linear(self.num_point, self.num_bit)\n","        self.lcode_fc = nn.Linear(self.num_lc_bit, int(self.num_point / self.chunk_ratio))\n","        self.lcode_fc_back = nn.Linear(int(self.num_point / self.chunk_ratio), self.num_lc_bit)\n","        self.discriminator = Discriminator(config_path)\n","        self.attack_layer = AttackLayer(config_path, device)\n","        self.balance_block = BalanceBlock(config_path)\n","\n","    # All other methods (stft, istft, embed_msg, etc.) are inherited from IDEAW\n","    # No need to redefine them!\n","\n","print(\"‚úì IDEAW_Plus model created (inherits from IDEAW)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bv4jTbj-EMZn","executionInfo":{"status":"ok","timestamp":1763303297378,"user_tz":-330,"elapsed":26,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"5b40a84b-d0fd-4839-925c-e2b4281cf38d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì IDEAW_Plus model created (inherits from IDEAW)\n"]}]},{"cell_type":"markdown","source":["\n","### Cell 5: Initialize Model (Baseline or Plus)"],"metadata":{"id":"_udwpGXHEUC1"}},{"cell_type":"code","source":["# ============================================\n","# INITIALIZE MODEL BASED ON FLAG\n","# ============================================\n","from models.ideaw import IDEAW\n","\n","model_config_path = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW/models/config.yaml'\n","\n","if USE_IDEAW_PLUS:\n","    # Use IDEAW-Plus with all improvements\n","    model = IDEAW_Plus(model_config_path, device).to(device)\n","    model_name = \"IDEAW-Plus\"\n","else:\n","    # Use baseline IDEAW\n","    model = IDEAW(model_config_path, device).to(device)\n","    model_name = \"IDEAW (Baseline)\"\n","\n","# Count parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"\\n{'='*50}\")\n","print(f\"MODEL: {model_name}\")\n","print(f\"{'='*50}\")\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n","\n","if USE_IDEAW_PLUS:\n","    attention_params = sum(p.numel() for p in model.hinet_1.innbs[0].attention.parameters())\n","    num_blocks = model.num_inn_1 + model.num_inn_2\n","    overhead = attention_params * num_blocks\n","    print(f\"\\nAttention overhead:\")\n","    print(f\"  Per block: {attention_params:,} params\")\n","    print(f\"  Total: {overhead:,} params ({overhead/total_params*100:.2f}%)\")\n","\n","print(f\"{'='*50}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69CffgOcEUq1","executionInfo":{"status":"ok","timestamp":1763303715112,"user_tz":-330,"elapsed":517,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"13d03436-3a0b-4a48-ecd0-a6f9ae6bbd42"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Working directory: /content/drive/MyDrive/audio-watermarking-demo/research/IDEAW\n","Initializing solver...\n","[IDEAW]infinite dataloader built\n","[IDEAW]model built\n","[IDEAW]total parameter count: 8425023\n","[IDEAW]optimizers built\n","‚úì Solver initialized with IDEAW-Plus\n","‚úì Optimizers rebuilt for IDEAW-Plus\n","\n","Starting training...\n","==================================================\n"]}]},{"cell_type":"code","source":["# ============================================\n","# TEST FORWARD PASS\n","# ============================================\n","import yaml\n","\n","with open(model_config_path) as f:\n","    test_config = yaml.load(f, Loader=yaml.FullLoader)\n","\n","num_point = test_config['IDEAW']['num_point']\n","num_bit = test_config['IDEAW']['num_bit']\n","num_lc_bit = test_config['IDEAW']['num_lc_bit']\n","\n","# Create test data\n","test_audio = torch.randn(1, num_point).to(device) * 0.1\n","test_msg = (torch.randint(0, 2, (1, num_bit), dtype=torch.float32) * 2 - 1).to(device)\n","test_lcode = (torch.randint(0, 2, (1, num_lc_bit), dtype=torch.float32) * 2 - 1).to(device)\n","\n","print(f\"Test data shapes:\")\n","print(f\"  Audio: {test_audio.shape}\")\n","print(f\"  Message: {test_msg.shape}\")\n","print(f\"  Lcode: {test_lcode.shape}\")\n","\n","# Forward pass\n","with torch.no_grad():\n","    outputs = model(test_audio, test_msg, test_lcode, False, False)\n","\n","    print(f\"\\n‚úÖ Forward pass successful!\")\n","    print(f\"  Watermarked audio shape: {outputs[2].shape}\")\n","    print(f\"  Audio range: [{outputs[2].min():.4f}, {outputs[2].max():.4f}]\")\n","\n","    # Check message extraction\n","    msg_acc = (torch.sign(outputs[5]) == test_msg).float().mean().item()\n","    lcode_acc = (torch.sign(outputs[6]) == test_lcode).float().mean().item()\n","    print(f\"  Message accuracy: {msg_acc*100:.1f}%\")\n","    print(f\"  Lcode accuracy: {lcode_acc*100:.1f}%\")\n","\n","print(f\"\\n‚úÖ {model_name} ready for training!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qa1A0BeyEbTs","executionInfo":{"status":"ok","timestamp":1763303395433,"user_tz":-330,"elapsed":28008,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"052f7230-74d6-4971-99b0-7c53f3b1d258"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Test data shapes:\n","  Audio: torch.Size([1, 16000])\n","  Message: torch.Size([1, 46])\n","  Lcode: torch.Size([1, 10])\n","\n","‚úÖ Forward pass successful!\n","  Watermarked audio shape: torch.Size([1, 16000])\n","  Audio range: [-0.0000, 0.0000]\n","  Message accuracy: 43.5%\n","  Lcode accuracy: 40.0%\n","\n","‚úÖ IDEAW-Plus ready for training!\n"]}]},{"cell_type":"code","source":["# ============================================\n","# INITIALIZE PERCEPTUAL LOSS\n","# ============================================\n","\n","if USE_IDEAW_PLUS:\n","    perceptual_loss_fn = PerceptualLoss(n_fft=1024, hop_length=256).to(device)\n","    print(\"‚úì Perceptual loss initialized\")\n","    print(\"  This will be added to the training loss\")\n","else:\n","    perceptual_loss_fn = None\n","    print(\"‚úì Using standard loss (no perceptual loss)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hPRtvRoEjym","executionInfo":{"status":"ok","timestamp":1763303525316,"user_tz":-330,"elapsed":63,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"16873805-83d3-47e9-c86b-644e71081a28"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Perceptual loss initialized\n","  This will be added to the training loss\n"]}]},{"cell_type":"markdown","source":["## 5. Train Model"],"metadata":{"id":"train-header"}},{"cell_type":"code","source":["# Initialize solver - use Drive path\n","import sys\n","import os\n","import argparse\n","\n","# Change to IDEAW directory on Drive\n","IDEAW_PATH = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW'\n","os.chdir(IDEAW_PATH)\n","sys.path.insert(0, IDEAW_PATH)\n","\n","print(f\"‚úì Working directory: {os.getcwd()}\")\n","\n","from solver import Solver\n","\n","# Create args object\n","args = argparse.Namespace(\n","    device=device,\n","    pickle_path=PICKLE_PATH,\n","    train_config='./config.yaml',\n","    store_model_path=f'{CHECKPOINT_PATH}/',\n","    load_model=False,  # Set to True to resume training\n","    load_model_path=f'{CHECKPOINT_PATH}/stage_I/',\n","    summary_steps=10,\n","    save_steps=SAVE_EVERY\n",")\n","\n","config_data_path = './data/config.yaml'\n","config_model_path = './models/config.yaml'\n","\n","print(\"Initializing solver...\")\n","\n","# MODIFICATION: Pass our model to solver\n","solver = Solver(config_data_path, config_model_path, args)\n","\n","# REPLACE solver's model with our model (baseline or plus)\n","solver.model = model\n","solver.model.to(device)\n","\n","# Reinitialize optimizers for the new model using solver's config\n","lr1 = eval(solver.config_t[\"train\"][\"lr1\"])\n","lr2 = eval(solver.config_t[\"train\"][\"lr2\"])\n","beta1 = solver.config_t[\"train\"][\"beta1\"]\n","beta2 = solver.config_t[\"train\"][\"beta2\"]\n","eps = eval(solver.config_t[\"train\"][\"eps\"])\n","weight_decay = eval(solver.config_t[\"train\"][\"weight_decay\"])\n","\n","# Rebuild optimizers with our model\n","param_hinet1 = list(filter(lambda p: p.requires_grad, solver.model.hinet_1.parameters()))\n","param_hinet2 = list(filter(lambda p: p.requires_grad, solver.model.hinet_2.parameters()))\n","param_discr = list(filter(lambda p: p.requires_grad, solver.model.discriminator.parameters()))\n","param_att = list(filter(lambda p: p.requires_grad, solver.model.attack_layer.parameters()))\n","param_balance = list(filter(lambda p: p.requires_grad, solver.model.balance_block.parameters()))\n","\n","solver.optim_I = torch.optim.Adam(\n","    param_hinet1 + param_hinet2,\n","    lr=lr1,\n","    betas=(beta1, beta2),\n","    eps=eps,\n","    weight_decay=weight_decay,\n",")\n","solver.optim_II = torch.optim.Adam(\n","    param_discr + param_att + param_balance,\n","    lr=lr2,\n","    betas=(beta1, beta2),\n","    eps=eps,\n","    weight_decay=weight_decay,\n",")\n","\n","print(f\"‚úì Solver initialized with {model_name}\")\n","print(f\"‚úì Optimizers rebuilt for {model_name}\")\n","print(\"\\nStarting training...\")\n","print(\"=\"*50)"],"metadata":{"id":"init-solver","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763303790926,"user_tz":-330,"elapsed":511,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"3858d905-0145-47d6-ca5f-0c3cd508b211"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì Working directory: /content/drive/MyDrive/audio-watermarking-demo/research/IDEAW\n","Initializing solver...\n","[IDEAW]infinite dataloader built\n","[IDEAW]model built\n","[IDEAW]total parameter count: 8425023\n","[IDEAW]optimizers built\n","‚úì Solver initialized with IDEAW-Plus\n","‚úì Optimizers rebuilt for IDEAW-Plus\n","\n","Starting training...\n","==================================================\n"]}]},{"cell_type":"code","source":["# Training loop\n","import time\n","\n","start_time = time.time()\n","\n","try:\n","    solver.train(NUM_ITERATIONS)\n","\n","    training_time = time.time() - start_time\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"‚úÖ TRAINING COMPLETE\")\n","    print(\"=\"*50)\n","    print(f\"Time: {training_time/60:.1f} minutes\")\n","    print(f\"Checkpoints saved to: {CHECKPOINT_PATH}\")\n","\n","except KeyboardInterrupt:\n","    print(\"\\n‚ö†Ô∏è  Training interrupted\")\n","    print(\"Checkpoints saved.\")\n","\n","except Exception as e:\n","    print(f\"\\n‚ùå Error: {e}\")\n","    import traceback\n","    traceback.print_exc()"],"metadata":{"id":"train-loop","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8638096d-c4c5-44e8-f620-60b59c60e697"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[IDEAW]starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n"]}]},{"cell_type":"code","source":["# Simpler test - just check if checkpoint loads and model structure is correct\n","import sys\n","import os\n","import torch\n","\n","sys.path.insert(0, '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW')\n","\n","from models.ideaw import IDEAW\n","\n","# Initialize model\n","model_config_path = '/content/drive/MyDrive/audio-watermarking-demo/research/IDEAW/models/config.yaml'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","ideaw = IDEAW(model_config_path, device)\n","ideaw = ideaw.to(device)\n","print(\"‚úì IDEAW model initialized\")\n","\n","# Load checkpoint\n","checkpoint_path = '/content/drive/MyDrive/audio-watermarking-demo/checkpoints/stage_I/ideaw.ckpt'\n","\n","if os.path.exists(checkpoint_path):\n","    print(f\"Loading checkpoint from: {checkpoint_path}\")\n","    checkpoint = torch.load(checkpoint_path)\n","    ideaw.load_state_dict(checkpoint)\n","    ideaw.eval()\n","    print(\"‚úì Checkpoint loaded successfully\")\n","\n","    # Count parameters\n","    total_params = sum(p.numel() for p in ideaw.parameters())\n","    print(f\"‚úì Model parameters: {total_params:,}\")\n","\n","    print(\"\\n‚úÖ CHECKPOINT TEST PASSED!\")\n","    print(\"The model checkpoint is valid and can be loaded.\")\n","    print(\"\\nTo properly test watermarking:\")\n","    print(\"1. Use the standalone_demo.py script\")\n","    print(\"2. Or continue training to improve accuracy\")\n","\n","else:\n","    print(f\"‚ùå Checkpoint not found at {checkpoint_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gHVAu8oMYcp-","executionInfo":{"status":"ok","timestamp":1763291977373,"user_tz":-330,"elapsed":667,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"0c441e8e-4023-4ccd-9461-ff0a864c51d5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì IDEAW model initialized\n","Loading checkpoint from: /content/drive/MyDrive/audio-watermarking-demo/checkpoints/stage_I/ideaw.ckpt\n","‚úì Checkpoint loaded successfully\n","‚úì Model parameters: 8,425,023\n","\n","‚úÖ CHECKPOINT TEST PASSED!\n","The model checkpoint is valid and can be loaded.\n","\n","To properly test watermarking:\n","1. Use the standalone_demo.py script\n","2. Or continue training to improve accuracy\n"]}]},{"cell_type":"markdown","source":["## 6. Visualize Training Results"],"metadata":{"id":"viz-header"}},{"cell_type":"code","source":["# Plot training curves\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","log_file = f'{RESULTS_PATH}/training_log.csv'\n","\n","if os.path.exists(log_file):\n","    df = pd.read_csv(log_file)\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # Loss\n","    axes[0, 0].plot(df['epoch'], df['loss'])\n","    axes[0, 0].set_xlabel('Epoch')\n","    axes[0, 0].set_ylabel('Loss')\n","    axes[0, 0].set_title('Training Loss')\n","    axes[0, 0].grid(True)\n","\n","    # SNR\n","    axes[0, 1].plot(df['epoch'], df['snr'])\n","    axes[0, 1].set_xlabel('Epoch')\n","    axes[0, 1].set_ylabel('SNR (dB)')\n","    axes[0, 1].set_title('Signal-to-Noise Ratio')\n","    axes[0, 1].grid(True)\n","\n","    # Accuracy\n","    axes[1, 0].plot(df['epoch'], df['accuracy'])\n","    axes[1, 0].set_xlabel('Epoch')\n","    axes[1, 0].set_ylabel('Accuracy (%)')\n","    axes[1, 0].set_title('Watermark Accuracy')\n","    axes[1, 0].grid(True)\n","\n","    # Learning rate\n","    if 'learning_rate' in df.columns:\n","        axes[1, 1].plot(df['epoch'], df['learning_rate'])\n","        axes[1, 1].set_xlabel('Epoch')\n","        axes[1, 1].set_ylabel('Learning Rate')\n","        axes[1, 1].set_title('Learning Rate Schedule')\n","        axes[1, 1].set_yscale('log')\n","        axes[1, 1].grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{RESULTS_PATH}/training_curves.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"‚úì Training curves saved to:\", f'{RESULTS_PATH}/training_curves.png')\n","\n","    # Print final metrics\n","    print(\"\\nFinal Metrics:\")\n","    print(f\"  Loss: {df['loss'].iloc[-1]:.4f}\")\n","    print(f\"  SNR: {df['snr'].iloc[-1]:.2f} dB\")\n","    print(f\"  Accuracy: {df['accuracy'].iloc[-1]:.2f}%\")\n","else:\n","    print(\"‚ö†Ô∏è No training log found\")"],"metadata":{"id":"visualize","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763291709210,"user_tz":-330,"elapsed":44,"user":{"displayName":"Abdullah yassir","userId":"06014537306864003482"}},"outputId":"7252d794-0acf-49f4-96d1-5c184a6e6957"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["‚ö†Ô∏è No training log found\n"]}]},{"cell_type":"markdown","source":["## 7. Test Trained Model"],"metadata":{"id":"test-header"}},{"cell_type":"code","source":["# Load best checkpoint\n","best_checkpoint = f'{CHECKPOINT_PATH}/best_model.pth'\n","\n","if os.path.exists(best_checkpoint):\n","    print(\"Loading best model...\")\n","    checkpoint = torch.load(best_checkpoint)\n","    ideaw.load_state_dict(checkpoint['model_state_dict'])\n","    ideaw.eval()\n","    print(\"‚úì Best model loaded\")\n","\n","    # Test on sample audio\n","    import librosa\n","    import numpy as np\n","\n","    # Load test audio\n","    test_audio_path = f'{LOCAL_DATA_PATH}/val/test_audio.wav'  # Update with your test file\n","\n","    if os.path.exists(test_audio_path):\n","        audio, sr = librosa.load(test_audio_path, sr=16000)\n","        audio_tensor = torch.FloatTensor(audio).unsqueeze(0).to(device)\n","\n","        # Generate random message and location code\n","        message = torch.randint(0, 2, (1, 16), dtype=torch.float32).to(device)\n","        lcode = torch.randint(0, 2, (1, 10), dtype=torch.float32).to(device)\n","\n","        with torch.no_grad():\n","            # Embed\n","            audio_wmd1, _ = ideaw.embed_msg(audio_tensor, message)\n","            audio_wmd2, _ = ideaw.embed_lcode(audio_wmd1, lcode)\n","\n","            # Extract\n","            mid_stft, lcode_extracted = ideaw.extract_lcode(audio_wmd2)\n","            message_extracted = ideaw.extract_msg(mid_stft)\n","\n","            # Calculate accuracy\n","            msg_acc = ((message_extracted > 0.5).float() == message).float().mean().item() * 100\n","            lcode_acc = ((lcode_extracted > 0.5).float() == lcode).float().mean().item() * 100\n","\n","            print(f\"\\nTest Results:\")\n","            print(f\"  Message accuracy: {msg_acc:.2f}%\")\n","            print(f\"  Location code accuracy: {lcode_acc:.2f}%\")\n","    else:\n","        print(f\"‚ö†Ô∏è Test audio not found at {test_audio_path}\")\n","else:\n","    print(f\"‚ö†Ô∏è Checkpoint not found at {best_checkpoint}\")"],"metadata":{"id":"test-model"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Download Results"],"metadata":{"id":"download-header"}},{"cell_type":"code","source":["# Zip checkpoints and results\n","!zip -r checkpoints.zip {CHECKPOINT_PATH}\n","!zip -r results.zip {RESULTS_PATH}\n","\n","print(\"‚úì Files zipped\")\n","print(\"\\nYou can download:\")\n","print(\"  1. checkpoints.zip - Trained model weights\")\n","print(\"  2. results.zip - Training logs and plots\")\n","print(\"\\nOr access them directly from Google Drive at:\")\n","print(f\"  {DRIVE_PATH}\")"],"metadata":{"id":"download"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional: Download directly from Colab\n","from google.colab import files\n","\n","# Uncomment to download\n","# files.download('checkpoints.zip')\n","# files.download('results.zip')"],"metadata":{"id":"download-direct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. Push Code Updates to GitHub (Optional)"],"metadata":{"id":"push-github-header"}},{"cell_type":"code","source":["# If you made code changes in Colab, push them back to GitHub\n","\n","# Configure git (first time only)\n","!git config --global user.email \"your.email@example.com\"\n","!git config --global user.name \"Your Name\"\n","\n","# Check what changed\n","!git status\n","\n","# Add, commit, and push (uncomment to use)\n","# !git add .\n","# !git commit -m \"Updated training code from Colab\"\n","# !git push\n","\n","print(\"\\nNote: You'll need to authenticate with GitHub token if pushing\")\n","print(\"Generate token at: https://github.com/settings/tokens\")"],"metadata":{"id":"push-github"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10. Pull Latest Code Updates (Optional)"],"metadata":{"id":"pull-github-header"}},{"cell_type":"code","source":["# If you updated code on your local machine, pull latest changes\n","!git pull origin main\n","\n","print(\"‚úì Code updated from GitHub\")"],"metadata":{"id":"pull-github"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 11. Keep Session Alive (Optional)\n","\n","Run this JavaScript in your browser console to prevent disconnection:\n","\n","```javascript\n","function KeepAlive() {\n","    console.log(\"Keeping session alive...\");\n","    document.querySelector(\"colab-connect-button\").click();\n","}\n","setInterval(KeepAlive, 60000);\n","```"],"metadata":{"id":"keep-alive"}}]}